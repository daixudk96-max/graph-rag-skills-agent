#!/usr/bin/env python3
"""
Transcript Scraper

å°† transcript æ–‡ä»¶è½¬æ¢ä¸º Claude AI Skillã€‚
è¿™æ˜¯é¢å‘ unified_scraper çš„ä¸»æ¥å£ï¼Œéµå¾ªä¸ doc_scraperã€github_scraperã€pdf_scraper ç›¸åŒçš„æ¨¡å¼ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
1. è¯»å–å¹¶è§£æ transcript æ–‡ä»¶ï¼ˆ.srt, .vtt, .txt, .mdï¼‰
2. ç”Ÿæˆç»“æ„åŒ–çš„ Skill è¾“å‡ºï¼ˆåŒ…å«æ‘˜è¦ã€å…³é”®ç‚¹ã€ç»ƒä¹ é¢˜ï¼‰
3. æ”¯æŒ CLI å•æ–‡ä»¶å¤„ç†å’Œé…ç½®æ–‡ä»¶æ‰¹é‡å¤„ç†
"""

import os
import sys
import json
import logging
import argparse
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime

# å¯¼å…¥æœ¬åœ°æ¨¡å—
try:
    from transcript_parser import TranscriptParser, parse_transcript
    from data_types import Lesson
except ImportError:
    from skill_seekers.cli.transcript_parser import TranscriptParser, parse_transcript
    from skill_seekers.cli.data_types import Lesson

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class TranscriptScraper:
    """
    Transcript æ–‡ä»¶åˆ° Skill çš„è½¬æ¢å™¨ã€‚
    
    éµå¾ª Skill_Seekers æ¶æ„ï¼Œæä¾›ä¸å…¶ä»– scraper ä¸€è‡´çš„æ¥å£ï¼š
    - fetch(): è·å–å’Œè§£ææºå†…å®¹
    - clean(): æ¸…ç†å’Œè§„èŒƒåŒ–å†…å®¹ï¼ˆåœ¨ parser ä¸­å·²å®Œæˆï¼‰
    - chunk(): å¤„ç†è¶…é•¿å†…å®¹ï¼ˆå¤§å¤šæ•°æƒ…å†µä¸‹ä¸éœ€è¦ï¼‰
    - scrape(): ä¸»å…¥å£ï¼Œæ‰§è¡Œå®Œæ•´æµç¨‹
    """
    
    # Skill è¾“å‡ºçš„æ ‡å‡† Prompt æ¨¡æ¿
    SKILL_PROMPT_TEMPLATE = """You are processing a course transcript. 
Your output MUST include the following sections:

1. **Summary**: A concise 2-3 paragraph overview of the lesson
2. **Key Concepts**: A bulleted list of the main concepts covered
3. **Practice Exercises**: Three distinct exercises to reinforce the material

Content to analyze:
---
{content}
---"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ– TranscriptScraperã€‚
        
        Args:
            config: é…ç½®å­—å…¸ï¼Œå¯ä»¥æ˜¯ï¼š
                - å®Œæ•´çš„ unified configï¼ˆåŒ…å« sources æ•°ç»„ï¼‰
                - å•ä¸ª transcript æºé…ç½®
                - ç®€å•é…ç½®ï¼ˆä»…åŒ…å« path å’Œ nameï¼‰
        """
        self.config = config
        
        # è§£æé…ç½®
        self.name = config.get('name', 'transcript_skill')
        self.paths = self._resolve_paths(config)
        
        # è¾“å‡ºç›®å½•
        self.output_dir = f"output/{self.name}"
        self.data_dir = f"output/{self.name}_data"
        
        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs(self.data_dir, exist_ok=True)
        
        # å­˜å‚¨è§£æåçš„è¯¾ç¨‹
        self.lessons: List[Lesson] = []
        self.scraped_data: Dict[str, Any] = {}
        
        logger.info(f"TranscriptScraper initialized: {len(self.paths)} files to process")
    
    def _resolve_paths(self, config: Dict[str, Any]) -> List[str]:
        """ä»é…ç½®ä¸­è§£ææ‰€æœ‰ transcript æ–‡ä»¶è·¯å¾„ã€‚"""
        paths = []
        
        # å•æ–‡ä»¶è·¯å¾„
        if 'path' in config:
            paths.append(config['path'])
        
        # å¤šæ–‡ä»¶è·¯å¾„æ•°ç»„
        if 'paths' in config:
            paths.extend(config['paths'])
        
        # ç›®å½•æ‰«æ
        if 'directory' in config:
            dir_path = Path(config['directory'])
            patterns = config.get('patterns', ['*.srt', '*.vtt', '*.txt'])
            for pattern in patterns:
                paths.extend(str(p) for p in dir_path.glob(pattern))
        
        # å»é‡å¹¶éªŒè¯
        unique_paths = list(dict.fromkeys(paths))
        valid_paths = [p for p in unique_paths if Path(p).exists()]
        
        if len(valid_paths) < len(unique_paths):
            missing = set(unique_paths) - set(valid_paths)
            logger.warning(f"Some files not found: {missing}")
        
        return valid_paths
    
    def fetch(self) -> List[Lesson]:
        """
        è·å–å¹¶è§£ææ‰€æœ‰ transcript æ–‡ä»¶ã€‚
        
        Returns:
            è§£æåçš„ Lesson å¯¹è±¡åˆ—è¡¨
        """
        logger.info("Fetching and parsing transcripts...")
        
        for path in self.paths:
            try:
                lesson = parse_transcript(path)
                self.lessons.append(lesson)
                logger.info(f"âœ… Parsed: {lesson.title} ({lesson.word_count()} words)")
            except Exception as e:
                logger.error(f"âŒ Failed to parse {path}: {e}")
        
        logger.info(f"Successfully parsed {len(self.lessons)}/{len(self.paths)} files")
        return self.lessons
    
    def clean(self) -> List[Lesson]:
        """
        æ¸…ç†å’Œè§„èŒƒåŒ–å†…å®¹ã€‚
        
        åœ¨ TranscriptParser ä¸­å·²å®Œæˆå¤§éƒ¨åˆ†æ¸…ç†å·¥ä½œï¼Œ
        è¿™é‡Œå¯ä»¥è¿›è¡Œé¢å¤–çš„åå¤„ç†ã€‚
        
        Returns:
            æ¸…ç†åçš„ Lesson åˆ—è¡¨
        """
        # TranscriptParser å·²å¤„ç†äº†æ—¶é—´æˆ³ç§»é™¤å’Œè¡Œåˆå¹¶
        # è¿™é‡Œå¯ä»¥æ·»åŠ é¢å¤–çš„æ¸…ç†é€»è¾‘
        return self.lessons
    
    def chunk(self, max_chars: int = 150000) -> List[Lesson]:
        """
        åˆ†å‰²è¿‡é•¿çš„å†…å®¹ã€‚
        
        Claude 200k context é€šå¸¸è¶³å¤Ÿå¤„ç†å•ä¸ªè¯¾ç¨‹ï¼Œ
        ä½†å¦‚æœå†…å®¹è¿‡é•¿ï¼Œéœ€è¦åˆ†å‰²ã€‚
        
        Args:
            max_chars: å•ä¸ª lesson çš„æœ€å¤§å­—ç¬¦æ•°
            
        Returns:
            å¯èƒ½åˆ†å‰²åçš„ Lesson åˆ—è¡¨
        """
        chunked_lessons = []
        
        for lesson in self.lessons:
            if lesson.char_count() <= max_chars:
                chunked_lessons.append(lesson)
            else:
                # åˆ†å‰²è¶…é•¿å†…å®¹
                chunks = self._split_content(lesson.content, max_chars)
                for i, chunk in enumerate(chunks):
                    chunked_lesson = Lesson(
                        title=f"{lesson.title} (Part {i+1})",
                        content=chunk,
                        source_path=lesson.source_path
                    )
                    chunked_lessons.append(chunked_lesson)
                logger.info(f"Split {lesson.title} into {len(chunks)} parts")
        
        self.lessons = chunked_lessons
        return self.lessons
    
    def _split_content(self, content: str, max_chars: int) -> List[str]:
        """åœ¨æ®µè½è¾¹ç•Œå¤„åˆ†å‰²å†…å®¹ã€‚"""
        paragraphs = content.split('\n\n')
        chunks = []
        current_chunk = []
        current_length = 0
        
        for para in paragraphs:
            para_len = len(para) + 2  # åŠ ä¸Šæ¢è¡Œç¬¦
            if current_length + para_len > max_chars and current_chunk:
                chunks.append('\n\n'.join(current_chunk))
                current_chunk = [para]
                current_length = para_len
            else:
                current_chunk.append(para)
                current_length += para_len
        
        if current_chunk:
            chunks.append('\n\n'.join(current_chunk))
        
        return chunks
    
    def scrape(self) -> Dict[str, Any]:
        """
        æ‰§è¡Œå®Œæ•´çš„ scraping æµç¨‹ã€‚
        
        Returns:
            åŒ…å«æ‰€æœ‰æå–æ•°æ®çš„å­—å…¸
        """
        logger.info("=" * 60)
        logger.info(f"Starting transcript scraping: {self.name}")
        logger.info("=" * 60)
        
        # Step 1: è·å–å’Œè§£æ
        self.fetch()
        
        # Step 2: æ¸…ç†
        self.clean()
        
        # Step 3: åˆ†å—ï¼ˆå¦‚æœ‰å¿…è¦ï¼‰
        self.chunk()
        
        # Step 4: æ„å»ºè¾“å‡ºæ•°æ®
        self.scraped_data = self._build_data()
        
        # Step 5: ä¿å­˜æ•°æ®
        self._save_data()
        
        logger.info(f"âœ… Scraping complete: {len(self.lessons)} lessons processed")
        return self.scraped_data
    
    def _build_data(self) -> Dict[str, Any]:
        """æ„å»ºæ ‡å‡†è¾“å‡ºæ•°æ®æ ¼å¼ã€‚"""
        lessons_data = []
        
        for lesson in self.lessons:
            lessons_data.append({
                'title': lesson.title,
                'content': lesson.content,
                'source_path': lesson.source_path,
                'word_count': lesson.word_count(),
                'char_count': lesson.char_count(),
                'sections': lesson.sections
            })
        
        return {
            'name': self.name,
            'type': 'transcript',
            'generated_at': datetime.now().isoformat(),
            'total_lessons': len(self.lessons),
            'total_words': sum(l.word_count() for l in self.lessons),
            'lessons': lessons_data,
            'prompt_template': self.SKILL_PROMPT_TEMPLATE
        }
    
    def _save_data(self):
        """ä¿å­˜æå–çš„æ•°æ®åˆ° JSON æ–‡ä»¶ã€‚"""
        data_file = os.path.join(self.data_dir, 'transcript_data.json')
        with open(data_file, 'w', encoding='utf-8') as f:
            json.dump(self.scraped_data, f, indent=2, ensure_ascii=False)
        logger.info(f"Data saved: {data_file}")
        
        # ä¿å­˜æ‘˜è¦æ–‡ä»¶ï¼ˆä¸å…¶ä»– scraper ä¿æŒä¸€è‡´ï¼‰
        summary = {
            'name': self.name,
            'type': 'transcript',
            'total_lessons': len(self.lessons),
            'total_words': sum(l.word_count() for l in self.lessons),
            'lessons': [
                {'title': l.title, 'words': l.word_count()}
                for l in self.lessons
            ]
        }
        summary_file = os.path.join(self.data_dir, 'summary.json')
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
    
    def build_skill(self) -> str:
        """
        ç”Ÿæˆ Skill æ–‡ä»¶ã€‚
        
        Returns:
            ç”Ÿæˆçš„ SKILL.md æ–‡ä»¶è·¯å¾„
        """
        if not self.scraped_data:
            self.scrape()
        
        skill_content = self._generate_skill_md()
        
        skill_path = os.path.join(self.output_dir, 'SKILL.md')
        with open(skill_path, 'w', encoding='utf-8') as f:
            f.write(skill_content)
        
        logger.info(f"Skill generated: {skill_path}")
        return skill_path
    
    def _generate_skill_md(self) -> str:
        """ç”Ÿæˆ SKILL.md å†…å®¹ï¼Œè°ƒç”¨ Claude API ç”Ÿæˆç»“æ„åŒ–è¾“å‡ºã€‚"""
        lines = [
            f"# {self.name}",
            "",
            "## Overview",
            "",
            f"This skill was generated from {len(self.lessons)} transcript(s).",
            f"Total word count: {sum(l.word_count() for l in self.lessons):,}",
            "",
        ]
        
        for lesson in self.lessons:
            lines.extend([
                f"## {lesson.title}",
                "",
                f"*Source: {lesson.source_path}*",
                "",
            ])
            
            # å°è¯•ä½¿ç”¨ Claude API ç”Ÿæˆç»“æ„åŒ–å†…å®¹
            enhanced_content = self._enhance_with_llm(lesson)
            
            if enhanced_content:
                lines.extend([enhanced_content, ""])
            else:
                # å¦‚æœ LLM è°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨çº¯å†…å®¹
                lines.extend([
                    "### Content",
                    "",
                    lesson.content,
                    ""
                ])
            
            lines.extend(["---", ""])
        
        return '\n'.join(lines)
    
    def _enhance_with_llm(self, lesson: Lesson) -> Optional[str]:
        """
        ä½¿ç”¨ Claude API ç”Ÿæˆç»“æ„åŒ–çš„ Skill å†…å®¹ã€‚
        
        Returns:
            å¢å¼ºåçš„ markdown å†…å®¹ï¼Œå¦‚æœå¤±è´¥è¿”å› None
        """
        try:
            import anthropic
        except ImportError:
            logger.warning("anthropic åŒ…æœªå®‰è£…ï¼Œè·³è¿‡ LLM å¢å¼ºã€‚ä½¿ç”¨ 'pip install anthropic' å®‰è£…ã€‚")
            return None
        
        api_key = os.environ.get('ANTHROPIC_API_KEY')
        if not api_key:
            logger.warning("æœªè®¾ç½® ANTHROPIC_API_KEY ç¯å¢ƒå˜é‡ï¼Œè·³è¿‡ LLM å¢å¼ºã€‚")
            return None
        
        logger.info(f"ğŸ¤– ä½¿ç”¨ Claude API å¢å¼ºå†…å®¹: {lesson.title}")
        
        # æ„å»º prompt
        prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è¯¾ç¨‹å†…å®¹åˆ†æå¸ˆã€‚è¯·åˆ†æä»¥ä¸‹è¯¾ç¨‹ transcriptï¼Œå¹¶ç”Ÿæˆç»“æ„åŒ–çš„å­¦ä¹ èµ„æ–™ã€‚

## è¯¾ç¨‹æ ‡é¢˜
{lesson.title}

## åŸå§‹ Transcript å†…å®¹
{lesson.content[:50000]}  # é™åˆ¶å†…å®¹é•¿åº¦

---

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼ˆä½¿ç”¨ä¸­æ–‡ï¼‰ï¼š

### ğŸ“ è¯¾ç¨‹æ‘˜è¦
ï¼ˆ2-3 æ®µç®€æ´çš„è¯¾ç¨‹æ¦‚è¿°ï¼Œå¸®åŠ©è¯»è€…å¿«é€Ÿäº†è§£è¯¾ç¨‹ä¸»é¢˜å’Œæ ¸å¿ƒä»·å€¼ï¼‰

### ğŸ¯ å…³é”®è¦ç‚¹
ï¼ˆç”¨åˆ—è¡¨å½¢å¼åˆ—å‡º 5-10 ä¸ªæ ¸å¿ƒæ¦‚å¿µå’Œå…³é”®çŸ¥è¯†ç‚¹ï¼‰

### ğŸ’¡ æ ¸å¿ƒæ¦‚å¿µè¯¦è§£
ï¼ˆå¯¹æœ€é‡è¦çš„ 3-5 ä¸ªæ¦‚å¿µè¿›è¡Œè¯¦ç»†è§£é‡Šï¼‰

### ğŸ“‹ å®è·µç»ƒä¹ 
ï¼ˆè®¾è®¡ 3 é“ç»ƒä¹ é¢˜ï¼Œå¸®åŠ©å·©å›ºæ‰€å­¦çŸ¥è¯†ï¼‰

### ğŸ”— å»¶ä¼¸å­¦ä¹ 
ï¼ˆæä¾› 2-3 ä¸ªè¿›ä¸€æ­¥å­¦ä¹ çš„å»ºè®®æ–¹å‘ï¼‰

è¯·ç¡®ä¿è¾“å‡ºå†…å®¹ï¼š
1. å‡†ç¡®æå–åŸæ–‡çš„æ ¸å¿ƒè§‚ç‚¹ï¼Œä¸è¦ç¼–é€ 
2. ä½¿ç”¨æ¸…æ™°çš„è¯­è¨€ç»„ç»‡ä¿¡æ¯
3. ä¿æŒç»“æ„åŒ–å’Œå¯è¯»æ€§"""

        try:
            client = anthropic.Anthropic(api_key=api_key)
            
            message = client.messages.create(
                model="claude-sonnet-4-20250514",
                max_tokens=4096,
                temperature=0.3,
                messages=[{
                    "role": "user",
                    "content": prompt
                }]
            )
            
            enhanced_content = message.content[0].text
            logger.info(f"âœ… LLM å¢å¼ºå®Œæˆ: {len(enhanced_content)} å­—ç¬¦")
            return enhanced_content
            
        except Exception as e:
            logger.error(f"âŒ Claude API è°ƒç”¨å¤±è´¥: {e}")
            return None


def main():
    """CLI å…¥å£ç‚¹ã€‚"""
    parser = argparse.ArgumentParser(
        description='Convert transcripts to Claude AI skills',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Process single file
  skill-seekers transcript --input lecture.srt
  
  # Process multiple files
  skill-seekers transcript --input lecture1.srt --input lecture2.srt
  
  # Process directory
  skill-seekers transcript --directory ./transcripts --patterns "*.srt" "*.vtt"
  
  # Use config file
  skill-seekers transcript --config transcript_config.json
        """
    )
    
    parser.add_argument('--input', '-i', action='append', dest='inputs',
                       help='Input transcript file (can specify multiple)')
    parser.add_argument('--directory', '-d',
                       help='Directory containing transcript files')
    parser.add_argument('--patterns', nargs='+', default=['*.srt', '*.vtt', '*.txt'],
                       help='File patterns to match (default: *.srt *.vtt *.txt)')
    parser.add_argument('--config', '-c',
                       help='Path to config JSON file')
    parser.add_argument('--name', '-n', default='transcript_skill',
                       help='Name for the output skill')
    
    args = parser.parse_args()
    
    # æ„å»ºé…ç½®
    if args.config:
        with open(args.config, 'r') as f:
            config = json.load(f)
    else:
        config = {
            'name': args.name,
            'paths': args.inputs or [],
            'patterns': args.patterns
        }
        if args.directory:
            config['directory'] = args.directory
    
    if not config.get('paths') and not config.get('directory') and not config.get('path'):
        parser.error("Must specify --input, --directory, or --config")
    
    # æ‰§è¡Œ scraping
    scraper = TranscriptScraper(config)
    scraper.scrape()
    scraper.build_skill()
    
    print(f"\nâœ… Skill generated: output/{config.get('name', 'transcript_skill')}/SKILL.md")


if __name__ == '__main__':
    main()
